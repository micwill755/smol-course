{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training Your Own LoRA Adapter\n",
    "\n",
    "This notebook will walk you through the complete process of creating, training, and using a LoRA (Low-Rank Adaptation) adapter for fine-tuning language models efficiently.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to set up LoRA configuration\n",
    "- How to prepare datasets for training\n",
    "- How to train a LoRA adapter\n",
    "- How to save and load your trained adapter\n",
    "- How to use your adapter for inference\n",
    "\n",
    "## Prerequisites\n",
    "Make sure you have the required packages installed:\n",
    "```bash\n",
    "pip install transformers peft datasets torch accelerate bitsandbytes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "PyTorch version: 2.7.1\n",
      "Transformers version: 4.56.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#from datasets import load_dataset, Dataset\n",
    "#from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "'''from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")'''\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Choose and Load Base Model\n",
    "\n",
    "We'll use a small model for this tutorial to ensure it runs on most hardware. You can replace this with any compatible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "#model_name = \"microsoft/DialoGPT-small\"  # Small model for tutorial\n",
    "model_name = \"./DialoGPT-small\" # using local\n",
    "# Alternative options:\n",
    "# model_name = \"HuggingFaceTB/SmolLM2-135M\"  # From the smol course\n",
    "# model_name = \"gpt2\"  # Classic choice\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with 8-bit quantization to save memory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure LoRA Parameters\n",
    "\n",
    "LoRA works by adding small trainable matrices to existing layers. Let's configure the LoRA parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # Causal language modeling\n",
    "    r=16,                          # Rank of adaptation (higher = more parameters)\n",
    "    lora_alpha=32,                 # LoRA scaling parameter (typically 2x the rank)\n",
    "    lora_dropout=0.1,              # Dropout for LoRA layers\n",
    "    target_modules=[\"c_attn\"],     # Target attention modules (varies by model)\n",
    "    bias=\"none\",                   # Whether to train bias parameters\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nLoRA configuration applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training Data\n",
    "\n",
    "For this tutorial, we'll create a simple dataset. In practice, you'd use your own domain-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple training dataset\n",
    "# This is just for demonstration - use your own data in practice\n",
    "training_texts = [\n",
    "    \"The weather today is beautiful and sunny.\",\n",
    "    \"I love learning about machine learning and AI.\",\n",
    "    \"LoRA is an efficient way to fine-tune large language models.\",\n",
    "    \"Python is a great programming language for data science.\",\n",
    "    \"Transformers have revolutionized natural language processing.\",\n",
    "    \"Fine-tuning allows models to adapt to specific tasks.\",\n",
    "    \"Parameter-efficient methods reduce computational costs.\",\n",
    "    \"Small language models can be very effective for focused tasks.\",\n",
    "    \"The Hugging Face ecosystem makes ML more accessible.\",\n",
    "    \"Open source AI tools democratize machine learning.\"\n",
    "]\n",
    "\n",
    "# Alternative: Load a real dataset\n",
    "# dataset = load_dataset(\"imdb\", split=\"train[:1000]\")  # Small subset\n",
    "# training_texts = dataset[\"text\"]\n",
    "\n",
    "print(f\"Training on {len(training_texts)} examples\")\n",
    "print(f\"Example text: {training_texts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the input texts\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,  # Adjust based on your data\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = Dataset.from_dict({\"text\": training_texts})\n",
    "\n",
    "# Tokenize\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset: {tokenized_dataset}\")\n",
    "print(f\"Example tokenized: {tokenized_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set Up Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "output_dir = \"./my-lora-adapter\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,    # Small batch size for tutorial\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,               # Number of training epochs\n",
    "    learning_rate=3e-4,               # Learning rate (higher for LoRA)\n",
    "    warmup_steps=10,                  # Warmup steps\n",
    "    logging_steps=5,                  # Log every N steps\n",
    "    save_steps=50,                    # Save checkpoint every N steps\n",
    "    save_total_limit=2,               # Keep only 2 checkpoints\n",
    "    prediction_loss_only=True,        # Only compute loss for evaluation\n",
    "    remove_unused_columns=False,      # Keep all columns\n",
    "    dataloader_pin_memory=False,      # Disable pin memory for compatibility\n",
    "    gradient_checkpointing=True,      # Save memory\n",
    "    fp16=True,                        # Use mixed precision\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Data Collator and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We're doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train the LoRA Adapter\n",
    "\n",
    "Now let's train our LoRA adapter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"This may take a few minutes depending on your hardware.\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save the LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"LoRA adapter saved to: {output_dir}\")\n",
    "\n",
    "# Check what files were saved\n",
    "saved_files = os.listdir(output_dir)\n",
    "print(f\"Saved files: {saved_files}\")\n",
    "\n",
    "# Check adapter size\n",
    "adapter_file = os.path.join(output_dir, \"adapter_model.safetensors\")\n",
    "if os.path.exists(adapter_file):\n",
    "    size_mb = os.path.getsize(adapter_file) / (1024 * 1024)\n",
    "    print(f\"Adapter size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Load and Test Your Trained Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model again (simulating a fresh start)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load your trained LoRA adapter\n",
    "model_with_adapter = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "print(\"LoRA adapter loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Test Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50):\n",
    "    \"\"\"Generate text using the model\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"The weather today is\",\n",
    "    \"Machine learning is\",\n",
    "    \"LoRA adapters are\"\n",
    "]\n",
    "\n",
    "print(\"Testing your trained model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(model_with_adapter, tokenizer, prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Compare with Base Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with base model (without adapter)\n",
    "print(\"Comparing with base model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_prompt = \"Machine learning is\"\n",
    "\n",
    "# Base model\n",
    "base_generated = generate_text(base_model, tokenizer, test_prompt)\n",
    "print(f\"Base model: {base_generated}\")\n",
    "\n",
    "# Model with adapter\n",
    "adapter_generated = generate_text(model_with_adapter, tokenizer, test_prompt)\n",
    "print(f\"With adapter: {adapter_generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Advanced - Multiple Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create multiple adapters for different tasks\n",
    "# and switch between them easily\n",
    "\n",
    "# Example: Create a second adapter for a different task\n",
    "# (This is just for demonstration)\n",
    "\n",
    "print(\"Creating a second adapter configuration...\")\n",
    "\n",
    "# Different LoRA config for a different task\n",
    "lora_config_2 = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,  # Different rank\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "print(\"You could train this on different data for a different task!\")\n",
    "print(\"Then switch between adapters as needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "1. ✅ Configured a LoRA adapter\n",
    "2. ✅ Prepared training data\n",
    "3. ✅ Trained your own LoRA adapter\n",
    "4. ✅ Saved and loaded the adapter\n",
    "5. ✅ Tested the adapted model\n",
    "\n",
    "### Key Takeaways:\n",
    "- LoRA adapters are much smaller than full model weights\n",
    "- Training is faster and requires less memory\n",
    "- You can create multiple adapters for different tasks\n",
    "- Adapters can be easily shared and distributed\n",
    "\n",
    "### Next Steps:\n",
    "1. **Try with your own data**: Replace the sample data with your domain-specific dataset\n",
    "2. **Experiment with parameters**: Try different `r`, `lora_alpha`, and `target_modules`\n",
    "3. **Use larger models**: Try with SmolLM2 or other models from the smol course\n",
    "4. **Combine with other techniques**: Explore combining LoRA with other PEFT methods\n",
    "5. **Deploy your adapter**: Use your adapter in production applications\n",
    "\n",
    "### Resources:\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [Smol Course Module 3](../3_parameter_efficient_finetuning/)\n",
    "\n",
    "Happy fine-tuning! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
