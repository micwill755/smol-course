{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f32de3f-b262-4ca0-adb8-afdb16f552ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46de79f1-db85-44d8-8040-1db88e6190c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Original model parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Load a small model for demonstration\n",
    "model_name = \"microsoft/DialoGPT-small\"  # Small model for quick demo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"üìä Original model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca45126-1e8a-4930-b294-1948ea66ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA - very simple setup\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                    # Small rank for demo\n",
    "    lora_alpha=8,          # 2x the rank (common practice)\n",
    "    lora_dropout=0.1,      # 10% dropout\n",
    "    bias=\"none\",           # Don't train bias\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Target specific attention layers\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4933e80-e2d4-4f62-9825-585e9eae807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py:2156: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA to the model\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "940c9cf1-bb79-4e66-b488-282c3f0ebd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Total parameters: 124,845,312\n",
      "üéØ Trainable parameters: 405,504\n",
      "üí° Percentage trainable: 0.32%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the difference\n",
    "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "print(f\"üìà Total parameters: {total_params:,}\")\n",
    "print(f\"üéØ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"üí° Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a68bd3-986b-4297-a050-93ad259bb7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç BEFORE Training:\n",
      "Prompt: 'Hello, how are you'\n",
      "Response: 'Hello, how are you? OP, this is a scam.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test generation BEFORE any training\n",
    "print(\"üîç BEFORE Training:\")\n",
    "test_prompt = \"Hello, how are you\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = peft_model.generate(\n",
    "        **inputs,\n",
    "        max_length=inputs['input_ids'].shape[1] + 10,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Prompt: '{test_prompt}'\")\n",
    "print(f\"Response: '{response}'\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcc19bfc-9eb4-425a-ba57-5b8a319eaa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß LoRA Modules Added:\n",
      "trainable params: 405,504 || all params: 124,845,312 || trainable%: 0.3248\n",
      "\n",
      "==================================================\n",
      "üí° Key Takeaways:\n",
      "‚Ä¢ LoRA only trains ~0.5% of parameters\n",
      "‚Ä¢ Original model weights stay frozen\n",
      "‚Ä¢ LoRA adds small 'adapter' layers\n",
      "‚Ä¢ Much faster and cheaper to train\n",
      "‚Ä¢ Can be easily swapped or removed\n"
     ]
    }
   ],
   "source": [
    "# Show what LoRA added to the model\n",
    "print(\"üîß LoRA Modules Added:\")\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üí° Key Takeaways:\")\n",
    "print(\"‚Ä¢ LoRA only trains ~0.5% of parameters\")\n",
    "print(\"‚Ä¢ Original model weights stay frozen\")\n",
    "print(\"‚Ä¢ LoRA adds small 'adapter' layers\")\n",
    "print(\"‚Ä¢ Much faster and cheaper to train\")\n",
    "print(\"‚Ä¢ Can be easily swapped or removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea8da5-87f5-40bc-bea0-f26900eba7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
